{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\chait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "import warnings\n",
    "import nltk\n",
    "import pyspark as ps\n",
    "from nltk.corpus import wordnet\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "findspark.init()\n",
    "\n",
    "#Import PySpark Tokenizer and Stopword Remover\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Get Bing Liu Dictionary, wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw')\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "############### Step 1 - Create Twitter Developer account and get all the keys and access tokens ###############\n",
    "#Twitter Authentication Keys\n",
    "consumer_key= '#################################'\n",
    "consumer_secret= '#############################################'\n",
    "access_token= '#############################################'\n",
    "access_token_secret= '######################################################'\n",
    "\n",
    "#Accessing twitter API\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "############### Step 2 - Get 1000 tweets ###############\n",
    "# Collect 1000 tweets\n",
    "date_since = \"2020-10-10\"\n",
    "new_search = \"#hospital\" + \" -filter:retweets\"\n",
    "tweets = tw.Cursor(api.search, q=new_search, lang=\"en\", since=date_since).items(1000)\n",
    "\n",
    "#convert iterable tweet object to list\n",
    "results = []\n",
    "for tweet in tweets : results.append(tweet)    \n",
    "\n",
    "# create SparkContext\n",
    "try:\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n",
    "\n",
    "dfTweetData = pd.DataFrame()\n",
    "dfTweetData['tweetID'] = [tweet.id for tweet in results]\n",
    "dfTweetData['tweetText'] = [tweet.text for tweet in results]\n",
    "#dfTweetData\n",
    "\n",
    "#create pyspark dataframe\n",
    "spark_df = sqlContext.createDataFrame(dfTweetData)\n",
    "#spark_df.show()\n",
    "\n",
    "#Removing ascii characters\n",
    "def ascii_ignore(x):\n",
    "    return x.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "ascii_udf = udf(ascii_ignore)\n",
    "spark_df_ascii = spark_df.withColumn(\"tweetText_new\", ascii_udf('tweetText'))\n",
    "\n",
    "############### Step 3 - Data Cleaning, Tokenizing, Sentiment Analysis ###############\n",
    "\n",
    "#Text Cleaning \n",
    "df_clean=spark_df_ascii.withColumn('tweetText_new', regexp_replace('tweetText_new', \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\").alias(\"replaced\")).withColumn('tweetText_new', regexp_replace('tweetText_new', \"[-_]\", \" \").alias(\"replaced\")).withColumn('tweetText_new', regexp_replace('tweetText_new', \"[\\\"]\", \"\").alias(\"replaced\")).withColumn('tweetText_new', regexp_replace('tweetText_new', \"[0-9]\", \"\").alias(\"replaced\")).withColumn('tweetText_new', regexp_replace('tweetText_new', \"[&$#@,:;.+!%/)(*?|]\", \"\").alias(\"replaced\")).withColumn('tweetText_new', regexp_replace('tweetText_new',  \"'\", \"\").alias(\"replaced\")).withColumn('tweetText_new', regexp_replace('tweetText_new',  \"\\n\", \" \").alias(\"replaced\")).withColumn('tweetText_new', regexp_replace('tweetText_new',  \" +\", \" \").alias(\"replaced\"))\n",
    "\n",
    "#Triming trailing spaces\n",
    "df_trim=df_clean.withColumn('tweetText_new', trim(df_clean.tweetText_new))\n",
    "\n",
    "#lower case\n",
    "df_lowercase = df_trim.select(\"*\", lower(col('tweetText_new')))\n",
    "\n",
    "#drop intermediate columns and rename an existing column\n",
    "dfFinal = df_lowercase.drop('tweetText_new').withColumnRenamed(\"lower(tweetText_new)\",\"cleaned_tweet_text\")\n",
    "\n",
    "#Tokenize the tweets\n",
    "tokenizer = Tokenizer(inputCol = \"cleaned_tweet_text\", outputCol = \"tokenized_words\")\n",
    "tokenizeTweetData = tokenizer.transform(dfFinal)\n",
    "#tokenizeTweetData.show()\n",
    "\n",
    "#remove stop words\n",
    "swr = StopWordsRemover(inputCol = tokenizer.getOutputCol(), outputCol = \"meaningful_words\")\n",
    "SWRemovedTweets = swr.transform(tokenizeTweetData)\n",
    "#SWRemovedTweets.show()\n",
    "FinalData = SWRemovedTweets.select(\"tweetID\",\"tweetText\",\"meaningful_words\")\n",
    "\n",
    "\n",
    "#Import  positive and negative list from Bing Liu dictionary\n",
    "from nltk.corpus import opinion_lexicon\n",
    "pos_list=list(set(opinion_lexicon.positive()))\n",
    "neg_list=list(set(opinion_lexicon.negative()))\n",
    "\n",
    "#Lemmatizer with POS tag\n",
    "def get_pos(word):\n",
    "    postag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    postag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return postag_dict.get(postag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def sentiment(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    senti=0\n",
    "    for word in words:\n",
    "        # initialize lemmatizer\n",
    "        lemma_word = lemmatizer.lemmatize(word, get_pos(word))\n",
    "        #calculate total score by positive and negative sentiment\n",
    "        if lemma_word in pos_list:\n",
    "            senti += 1\n",
    "        elif lemma_word in neg_list:\n",
    "            senti -= 1\n",
    "    return senti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Step 4 - Export Tweets and Sentiment Score to CSV ###############\n",
    "dfTweetScore = FinalData.select(\"*\").toPandas()\n",
    "dfTweetScore['Sentiment_Score'] = dfTweetScore['meaningful_words'].apply(sentiment)\n",
    "\n",
    "#Dropping and renmaing columns\n",
    "dfTweetScore = dfTweetScore.drop (['tweetID','meaningful_words'],axis=1)\n",
    "dfFinal = dfTweetScore.rename(columns={'tweetText': 'Tweet_content'})\n",
    "\n",
    "#write result to csv\n",
    "#dfFinal.to_csv(r'C:\\Users\\nikit\\Desktop\\IDS_561_BigData\\HW3\\Final_Code\\HW3_FinalResult.csv',index = False)\n",
    "dfFinal.to_csv(r'C:\\Users\\chait\\Desktop\\561- Big Data\\Assignment 3\\HW3_FinalResult.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
